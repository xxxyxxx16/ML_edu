{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polinominal regression, Ridge, Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Quadric trend\n",
    "* Forecasting\n",
    "* Bias-Variance Decomposition\n",
    "* Ridge regression\n",
    "* LAsso regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Polinominal_regression.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadric trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import *\n",
    "\n",
    "A = Matrix([[1,1,1],[1,2,4],[1,3,9],[1,4,16],[1,5,25],[1,6,36],[1,7,49],[1,8,64],[1,9,81],[1,10,100]])\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "    X^{T}:\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1\\\\1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10\\\\1 & 4 & 9 & 16 & 25 & 36 & 49 & 64 & 81 & 100\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[1, 1, 1,  1,  1,  1,  1,  1,  1,   1],\n",
       "[1, 2, 3,  4,  5,  6,  7,  8,  9,  10],\n",
       "[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(A.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Matrix([[0.59],[0.72],[0.93],[1.27],[2.58],[1.74],[2.07],[2.42],[2.73],[2.95]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "    y^{T}:\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0.59 & 0.72 & 0.93 & 1.27 & 2.58 & 1.74 & 2.07 & 2.42 & 2.73 & 2.95\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([[0.59, 0.72, 0.93, 1.27, 2.58, 1.74, 2.07, 2.42, 2.73, 2.95]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(y.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = A.transpose()\n",
    "d = (z*A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "(X^{T} \\cdot X)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}10 & 55 & 385\\\\55 & 385 & 3025\\\\385 & 3025 & 25333\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[ 10,   55,   385],\n",
       "[ 55,  385,  3025],\n",
       "[385, 3025, 25333]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = d.inv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "(X^{T} \\cdot X)^{-1}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}\\frac{83}{60} & - \\frac{21}{40} & \\frac{1}{24}\\\\- \\frac{21}{40} & \\frac{637}{2640} & - \\frac{1}{48}\\\\\\frac{1}{24} & - \\frac{1}{48} & \\frac{1}{528}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[ 83/60,   -21/40,  1/24],\n",
       "[-21/40, 637/2640, -1/48],\n",
       "[  1/24,    -1/48, 1/528]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = l*A.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "(X^{T} \\cdot X)^{-1} \\cdot X^{T}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}\\frac{9}{10} & \\frac{1}{2} & \\frac{11}{60} & - \\frac{1}{20} & - \\frac{1}{5} & - \\frac{4}{15} & - \\frac{1}{4} & - \\frac{3}{20} & \\frac{1}{30} & \\frac{3}{10}\\\\- \\frac{67}{220} & - \\frac{83}{660} & \\frac{1}{88} & \\frac{47}{440} & \\frac{53}{330} & \\frac{19}{110} & \\frac{63}{440} & \\frac{19}{264} & - \\frac{9}{220} & - \\frac{43}{220}\\\\\\frac{1}{44} & \\frac{1}{132} & - \\frac{1}{264} & - \\frac{1}{88} & - \\frac{1}{66} & - \\frac{1}{66} & - \\frac{1}{88} & - \\frac{1}{264} & \\frac{1}{132} & \\frac{1}{44}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[   9/10,     1/2,  11/60,  -1/20,   -1/5,  -4/15,   -1/4,  -3/20,   1/30,    3/10],\n",
       "[-67/220, -83/660,   1/88, 47/440, 53/330, 19/110, 63/440, 19/264, -9/220, -43/220],\n",
       "[   1/44,   1/132, -1/264,  -1/88,  -1/66,  -1/66,  -1/88, -1/264,  1/132,    1/44]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = f*y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\vec{w} = \\left(X^T X\\right)^{-1} X^T \\vec{y}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0.1135\\\\0.373189393939394\\\\-0.00950757575757577\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[              0.1135],\n",
       "[   0.373189393939394],\n",
       "[-0.00950757575757577]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trend forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "y_{11 tr} = \\hat{w}_1 + \\hat{w}_2 \\cdot 11 + \\hat{w}_3 \\cdot 11 ^{2}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y11tr =  3.06816666666666\n"
     ]
    }
   ],
   "source": [
    "y11tr = u[0]+u[1]*11+u[2]*11**2\n",
    "print('y11tr = ', y11tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "{\\hat{\\sigma}_{tr}}^2 = \\frac{1}{7} \\cdot \\sum^{10}_{k=1}\n",
    "({y_k - \\hat{w}_{1} - \\hat{w}_2 \\cdot k - \\hat{w}_3 \\cdot k^2})^2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RSS(b0, b1, b2, y):\n",
    "    RSS = 0\n",
    "    RSSi = 0\n",
    "    for i in range(1, len(y)+1):\n",
    "        RSSi = (y[i-1]-b0-b1*i - b2*i*i)**2\n",
    "        RSS += RSSi\n",
    "    D = RSS/(len(y)-2)\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D =  0.115620208333333\n"
     ]
    }
   ],
   "source": [
    "b0 = u[0]\n",
    "b1 = u[1]\n",
    "b2 = u[2]\n",
    "D = RSS(b0,b1,b2,y)\n",
    "print('D = ', D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expert forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y11 = 3.42\n",
    "s11 = 0.07\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "y_{for 11} = \\frac{\n",
    "    \\frac{y_{11}}{s_{11}^2}+\\frac{y_{11tr}}{ \\hat{\\sigma}_{tr}^2}\n",
    "    }\n",
    "    {\n",
    "    \\frac{1}{s_{11}^2}+\\frac{1}{ \\hat{\\sigma}_{tr}^2}\n",
    "    }\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint forecasting:  3.40569548329551\n"
     ]
    }
   ],
   "source": [
    "yfor11 = ((y11)/(s11**2)+(y11tr)/(D))/((1)/(s11**2)+(1)/(D))\n",
    "print(\"Joint forecasting: \", yfor11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/kashnitsky/topic-4-linear-models-part-1-ols\n",
    "\n",
    "## 3. Bias-Variance Decomposition\n",
    "\n",
    "Let's talk a little about the error properties of linear regression prediction (in fact, this discussion is valid for all machine learning algorithms). We just covered the following:\n",
    "\n",
    "- true value of the target variable is the sum of a deterministic function $f\\left(\\textbf{x}\\right)$ and random error $\\epsilon$: $y = f\\left(\\textbf{x}\\right) + \\epsilon$;\n",
    "- error is normally distributed with zero mean and some variance: $\\epsilon \\sim \\mathcal{N}\\left(0, \\sigma^2\\right)$;\n",
    "- true value of the target variable is also normally distributed: $y \\sim \\mathcal{N}\\left(f\\left(\\textbf{x}\\right), \\sigma^2\\right)$;\n",
    "- we try to approximate a deterministic but unknown function $f\\left(\\textbf{x}\\right)$ using a linear function of the covariates $\\widehat{f}\\left(\\textbf{x}\\right)$, which, in turn, is a point estimate of the function $f$ in function space (specifically, the family of linear functions that we have limited our space to), i.e. a random variable that has mean and variance.\n",
    "\n",
    "So, the error at the point $\\textbf{x}$ decomposes as follows:\n",
    "\n",
    "$$\\Large \\begin{array}{rcl} \n",
    "\\text{Err}\\left(\\textbf{x}\\right) &=& \\mathbb{E}\\left[\\left(y - \\widehat{f}\\left(\\textbf{x}\\right)\\right)^2\\right] \\\\\n",
    "&=& \\mathbb{E}\\left[y^2\\right] + \\mathbb{E}\\left[\\left(\\widehat{f}\\left(\\textbf{x}\\right)\\right)^2\\right] - 2\\mathbb{E}\\left[y\\widehat{f}\\left(\\textbf{x}\\right)\\right] \\\\\n",
    "&=& \\mathbb{E}\\left[y^2\\right] + \\mathbb{E}\\left[\\widehat{f}^2\\right] - 2\\mathbb{E}\\left[y\\widehat{f}\\right] \\\\\n",
    "\\end{array}$$\n",
    "\n",
    "For clarity, we will omit the notation of the argument of the functions. Let's consider each member separately. The first two are easily decomposed according to the formula $\\text{Var}\\left(z\\right) = \\mathbb{E}\\left[z^2\\right] - \\mathbb{E}\\left[z\\right]^2$:\n",
    "\n",
    "$$\\Large \\begin{array}{rcl} \n",
    "\\mathbb{E}\\left[y^2\\right] &=& \\text{Var}\\left(y\\right) + \\mathbb{E}\\left[y\\right]^2 = \\sigma^2 + f^2\\\\\n",
    "\\mathbb{E}\\left[\\widehat{f}^2\\right] &=& \\text{Var}\\left(\\widehat{f}\\right) + \\mathbb{E}\\left[\\widehat{f}\\right]^2 \\\\\n",
    "\\end{array}$$\n",
    "\n",
    "Note that:\n",
    "\n",
    "$$\\Large \\begin{array}{rcl} \n",
    "\\text{Var}\\left(y\\right) &=& \\mathbb{E}\\left[\\left(y - \\mathbb{E}\\left[y\\right]\\right)^2\\right] \\\\\n",
    "&=& \\mathbb{E}\\left[\\left(y - f\\right)^2\\right] \\\\\n",
    "&=& \\mathbb{E}\\left[\\left(f + \\epsilon - f\\right)^2\\right] \\\\\n",
    "&=& \\mathbb{E}\\left[\\epsilon^2\\right] = \\sigma^2\n",
    "\\end{array}$$\n",
    "\n",
    "$$\\Large \\mathbb{E}[y] = \\mathbb{E}[f + \\epsilon] = \\mathbb{E}[f] + \\mathbb{E}[\\epsilon] = f$$\n",
    "\n",
    "And finally, we get to the last term in the sum. Recall that the error and the target variable are independent of each other:\n",
    "\n",
    "$$\\Large \\begin{array}{rcl} \n",
    "\\mathbb{E}\\left[y\\widehat{f}\\right] &=& \\mathbb{E}\\left[\\left(f + \\epsilon\\right)\\widehat{f}\\right] \\\\\n",
    "&=& \\mathbb{E}\\left[f\\widehat{f}\\right] + \\mathbb{E}\\left[\\epsilon\\widehat{f}\\right] \\\\\n",
    "&=& f\\mathbb{E}\\left[\\widehat{f}\\right] + \\mathbb{E}\\left[\\epsilon\\right] \\mathbb{E}\\left[\\widehat{f}\\right]  = f\\mathbb{E}\\left[\\widehat{f}\\right]\n",
    "\\end{array}$$\n",
    "\n",
    "Finally, let's bring this all together:\n",
    "\n",
    "$$\\Large \\begin{array}{rcl} \n",
    "\\text{Err}\\left(\\textbf{x}\\right) &=& \\mathbb{E}\\left[\\left(y - \\widehat{f}\\left(\\textbf{x}\\right)\\right)^2\\right] \\\\\n",
    "&=& \\sigma^2 + f^2 + \\text{Var}\\left(\\widehat{f}\\right) + \\mathbb{E}\\left[\\widehat{f}\\right]^2 - 2f\\mathbb{E}\\left[\\widehat{f}\\right] \\\\\n",
    "&=& \\left(f - \\mathbb{E}\\left[\\widehat{f}\\right]\\right)^2 + \\text{Var}\\left(\\widehat{f}\\right) + \\sigma^2 \\\\\n",
    "&=& \\text{Bias}\\left(\\widehat{f}\\right)^2 + \\text{Var}\\left(\\widehat{f}\\right) + \\sigma^2\n",
    "\\end{array}$$\n",
    "\n",
    "With that, we have reached our ultimate goal -- the last formula tells us that the forecast error of any model of type $y = f\\left(\\textbf{x}\\right) + \\epsilon$ is composed of:\n",
    "\n",
    "- squared bias: $\\text{Bias}\\left(\\widehat{f}\\right)$ is the average error for all sets of data;\n",
    "- variance: $\\text{Var}\\left(\\widehat{f}\\right)$ is error variability, or by how much error will vary if we train the model on different sets of data;\n",
    "- irremovable error: $\\sigma^2$.\n",
    "\n",
    "While we cannot do anything with the $\\sigma^2$ term, we can influence the first two. Ideally, we'd like to negate both of these terms (upper left square of the picture), but, in practice, it is often necessary to balance between the biased and unstable estimates (high variance).\n",
    "\n",
    "<img src=\"https://habrastorage.org/files/281/108/1e9/2811081e9eda44d08f350be5a9deb564.png\" width=\"480\">\n",
    "\n",
    "Generally, as the model becomes more computational (e.g. when the number of free parameters grows), the variance (dispersion) of the estimate also increases, but bias decreases. Due to the fact that the training set is memorized completely instead of generalizing, small changes lead to unexpected results (overfitting). On the other side, if the model is too weak, it will not be able to learn the pattern, resulting in learning something different that is offset with respect to the right solution.\n",
    "\n",
    "<img src=\"https://habrastorage.org/webt/mp/jr/fo/mpjrfodzlknbpirv2rs8fxf8-rs.png\" width=\"480\">\n",
    "\n",
    "The Gauss-Markov theorem asserts that the OLS estimator of parameters of the linear model is the best for the class of linear unbiased estimator. This means that if there exists any other unbiased model $g$, from the same class of linear models, we can be sure that $Var\\left(\\widehat{f}\\right) \\leq Var\\left(g\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/mephistopheies/dds/blob/master/lr_040117/ipy/lecture.ipynb\n",
    "\n",
    "## L2 Regularization ##\n",
    "\n",
    "<center>or ridge regression</center>\n",
    "\n",
    "One of the symptoms of overfitting: function try to interpolate data (pass throught each point). Magnitude of weights is increasing with the degree of polynomial. What about to add some penalty term for big weights? So general case of regularization with penalty term is:\n",
    "$$\\large \\mathcal{L}_{reg} \\left(X, \\vec{y}, \\vec{w}\\right) = \\mathcal{L}\\left(X, \\vec{y}, \\vec{w}\\right) + \\lambda R\\left(\\vec{w}\\right)$$\n",
    "where:\n",
    "* $\\large \\lambda$ is regularization parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/mephistopheies/dds/blob/master/lr_040117/ipy/lecture.ipynb\n",
    "\n",
    "Lets penalize for large L2 norm of the weights vector:\n",
    "$$\\large R\\left(\\vec{w}\\right) = \\frac{1}{2} \\left\\| \\vec{w} \\right\\|_2^2 = \\frac{1}{2} \\sum_{j=1}^m w_j^2 = \\frac{1}{2} \\vec{w}^T \\vec{w}$$\n",
    "then new objective will be:\n",
    "$$\\large \\mathcal{L}\\left(X, \\vec{y}, \\vec{w} \\right) = \\frac{1}{2} \\left(\\vec{y} - X \\vec{w}\\right)^T \\left(\\vec{y} - X \\vec{w}\\right) + \\frac{\\lambda}{2} \\vec{w}^T \\vec{w}$$\n",
    "Lets find derivative wrt to $\\large \\vec{w}$:\n",
    "$$\\large \\begin{array}{rcl}\\Large \\frac{\\partial \\mathcal{L}}{\\partial \\vec{w}} &=& \\frac{\\partial}{\\partial \\vec{w}} \\left(\\frac{1}{2} \\left(\\vec{y} - X \\vec{w}\\right)^T \\left(\\vec{y} - X \\vec{w}\\right) + \\frac{\\lambda}{2} \\vec{w}^T \\vec{w}\\right) \\\\\n",
    "&=& \\frac{\\partial}{\\partial \\vec{w}}\\left( \\frac{1}{2} \\left( \\vec{y}^T \\vec{y} -2\\vec{y}^T X \\vec{w} + \\vec{w}^T X^T X \\vec{w}\\right) + \\frac{\\lambda}{2} \\vec{w}^T \\vec{w} \\right) \\\\\n",
    "&=& -X^T \\vec{y} + X^T X \\vec{w} + \\lambda \\vec{w}\n",
    "\\end{array}$$\n",
    "And final solution:\n",
    "$$\\large \\begin{array}{rcl} \\frac{\\partial \\mathcal{L}}{\\partial \\vec{w}} = 0 &\\Leftrightarrow& -X^T \\vec{y} + X^T X \\vec{w} + \\lambda \\vec{w} = 0 \\\\\n",
    "&\\Leftrightarrow& X^T X \\vec{w} + \\lambda \\vec{w} = X^T \\vec{y} \\\\\n",
    "&\\Leftrightarrow& \\left(X^T X + \\lambda E\\right) \\vec{w} = X^T \\vec{y} \\\\\n",
    "&\\Leftrightarrow& \\vec{w} = \\left(X^T X + \\lambda E\\right)^{-1} X^T \\vec{y}\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/kashnitsky/topic-4-linear-models-part-1-ols\n",
    "\n",
    "This type of regression is called ridge regression. The ridge is the diagonal matrix that we add to the $\\textbf{X}^\\text{T} \\textbf{X}$ matrix to ensure that we get a regular matrix as a result.\n",
    "\n",
    "<img src=\"https://habrastorage.org/files/452/f36/c79/452f36c79d7843109993de219dde7cd5.png\">\n",
    "\n",
    "Such a solution reduces dispersion but becomes biased because the norm of the vector of parameters is also minimized, which makes the solution shift towards zero. On the figure below, the OLS solution is at the intersection of the white dotted lines. Blue dots represent different solutions of ridge regression. It can be seen that by increasing the regularization parameter $\\lambda$, we shift the solution towards zero.\n",
    "\n",
    "\n",
    "<img src=\"https://habrastorage.org/files/fe5/709/fe6/fe5709fe61fd4f3b8832a61f5fa05b9c.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/mephistopheies/dds/blob/master/lr_040117/ipy/lecture.ipynb\n",
    "\n",
    "## L1 regularization ##\n",
    "<center>or LASSO (least absolute shrinkage and selection operator)</center>\n",
    "\n",
    "Lets penalty for large L1 norm of the weights vector:\n",
    "$$\\large R\\left(\\vec{w}\\right) = \\left\\| \\vec{w} \\right\\|_1 = \\sum_{j=1}^m \\left| w_j \\right|$$\n",
    "then new objective will be:\n",
    "$$\\large \\mathcal{L}\\left(X, \\vec{y}, \\vec{w} \\right) = \\frac{1}{2n} \\sum_{i=1}^n \\left(\\vec{x_i}^T \\vec{w} - y_i\\right)^2 + \\lambda \\sum_{j=1}^m \\left| w_j \\right|$$\n",
    "Unfortunately, in general, there is no explicit formula for optimal weights. We will use gradient descent to estimate optimal values of weights. Lets find gradient wrt weights:\n",
    "$$\\large \\frac{\\partial \\mathcal{L}}{\\partial w_j} = \\frac{1}{n}\\sum_{i=1}^n \\left(\\vec{x_i}^T \\vec{w} - y_i\\right) \\vec{x_i} + \\lambda \\text{sign}(\\vec{w})$$\n",
    "So update formula of weights is:\n",
    "$$\\large \\vec{w}_{\\text{new}} := \\vec{w} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial \\vec{w}}$$\n",
    "where:\n",
    "* $\\large \\alpha$ is step of gradient descent (learning rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
