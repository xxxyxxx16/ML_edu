{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML, Linear_regression, forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Type of ML tasks\n",
    "* Generalization\n",
    "* Linear regression\n",
    "    * OLS\n",
    "    * Gauss–Markov theorem\n",
    "    * Maximum Likelihood Estimation\n",
    "    * Practical task\n",
    "* Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/mephistopheies/dds/blob/master/lr_040117/ipy/lecture.ipynb\n",
    "\n",
    "## Type of ML tasks\n",
    "\n",
    "* *Supervised learning* - ML task of inferring a function $f: X \\rightarrow Y$ from labeled data; each sample is a pair of feature vector and some desired output value $D = \\left\\{ \\left( x_i, y_i \\right) \\right\\}_{i=1, \\ldots, n}$\n",
    "    * categorical - classification\n",
    "    * continuous - regression\n",
    "    * ordinal - ordinal regression, ranking\n",
    "        \n",
    "* *Unsupervised learning* - ML task of inferring a function to describe hidden structure from unlabeled data $D = \\left\\{ x_i \\right\\}_{i=1, \\ldots, n}$\n",
    "    * clustering - split data into several groups    \n",
    "    * dimensionality reduction - reduce number of features minimizing inforamation loss\n",
    "    * matrix complition - collaborative filtering, reccomender system\n",
    "    * semi-supervised learning - when you are given with small set of labeled data and large set of unlabeled\n",
    "\n",
    "* *Reinforcement learning* - ML task where agent learn optimal behaviour from his actions and response from environment\n",
    "    * differs from standard supervised learning in that correct input/output pairs are never presented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/mephistopheies/dds/blob/master/lr_040117/ipy/lecture.ipynb\n",
    "\n",
    "## Generalization\n",
    "\n",
    "Main disadvantage of ERM is that it is prone to overfitting.\n",
    "\n",
    "* We say that model has *generalization ability* if probability of error on test set (unseen data, which is not available during training) is small or at least can be predicted\n",
    "* *Overfitting* is bad phenomenon when your model is very good on train data, but very bad on test data. Such model doesn't have generalization ability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Linear_regression.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/mephistopheies/dds/blob/master/lr_040117/ipy/lecture.ipynb\n",
    "\n",
    "Lets restict set of hypothesis to the set of linear functions with $\\large \\left(m + 1\\right)$-dimensional argument, bias and one parameter for each feature ($\\large x_0 = 1$):\n",
    "$$\\large \\begin{array}{rcl} \\forall h \\in \\mathcal{H}, h\\left(\\vec{x}\\right) &=& w_0 x_0 + w_1 x_1 + w_2 x_2 + \\cdots + w_m x_m \\\\\n",
    "&=& \\sum_{i=0}^m w_i x_i \\\\\n",
    "&=& \\vec{x}^T \\vec{w}\n",
    "\\end{array}$$\n",
    "where:\n",
    "* $\\large \\vec{x} \\in \\mathbb{R}^{m + 1}$\n",
    "\n",
    "Optimization objective is to minimize Mean Square Error (MSE):\n",
    "$$\\large \\begin{array}{rcl}\\mathcal{L}\\left(X, \\vec{y}, \\vec{w} \\right) &=& \\frac{1}{2n} \\sum_{i=1}^n \\left(y_i - \\vec{x}_i^T \\vec{w}\\right)^2 \\\\\n",
    "&=& \\frac{1}{2n} \\left\\| \\vec{y} - X \\vec{w} \\right\\|_2^2 \\\\\n",
    "&=& \\frac{1}{2n} \\left(\\vec{y} - X \\vec{w}\\right)^T \\left(\\vec{y} - X \\vec{w}\\right) \\\\\n",
    "&=& \\frac{1}{2n} \\left(\\vec{y}^T - \\vec{w}^T X^T \\right) \\left(\\vec{y} - X \\vec{w}\\right) \\\\\n",
    "&=& \\frac{1}{2n} \\left(\\vec{y}^T \\vec{y} - \\vec{y}^T X \\vec{w} - \\vec{w}^T X^T \\vec{y} + \\vec{w}^T X^T X \\vec{w}\\right) \\\\\n",
    "&=& \\frac{1}{2n} \\left(\\vec{y}^T \\vec{y} - 2 \\vec{y}^T X \\vec{w} + \\vec{w}^T X^T X \\vec{w}\\right) \\\\\n",
    "&=& \\frac{1}{2n} \\left(\\vec{y}^T \\vec{y} - 2 \\vec{w}^T X^T \\vec{y} + \\vec{w}^T X^T X \\vec{w}\\right)\n",
    "\\end{array}$$\n",
    "where:\n",
    "* $\\large \\vec{w} \\in \\mathbb{R}^{m + 1}$\n",
    "* $\\large \\vec{y} \\in \\mathbb{R}^n$\n",
    "* $\\large X$ is $\\large n \\times m$ matrix, where each row is feature vector\n",
    "\n",
    "Matrix and vectors derivatives:\n",
    "$$\\large \\begin{array}{rcl} \\frac{\\partial}{\\partial x} x^T a &=& a \\\\ \\frac{\\partial}{\\partial x} x^T A x &=& \\left(A + A^T\\right)x  \\end{array} $$\n",
    "\n",
    "Lets infer learning algorithm, we will use the fact that square function is convex function:\n",
    "$$\\large \\begin{array}{rcl} \\frac{\\partial \\mathcal{L}}{\\partial \\vec{w}} &=& \\frac{\\partial}{\\partial \\vec{w}} \\frac{1}{2n} \\left( \\vec{y}^T \\vec{y} -2\\vec{w}^T X^T \\vec{y} + \\vec{w}^T X^T X \\vec{w}\\right) \\\\\n",
    "&=& \\frac{1}{2n} \\left(-2 X^T \\vec{y} + 2X^T X \\vec{w}\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "Now we can find the solution (ordinary least squares, OLS):\n",
    "$$\\large \\begin{array}{rcl} \\frac{\\partial \\mathcal{L}}{\\partial \\vec{w}} = 0 &\\Leftrightarrow& \\frac{1}{2n} \\left(-2 X^T \\vec{y} + 2X^T X \\vec{w}\\right) = 0 \\\\\n",
    "&\\Leftrightarrow& -X^T \\vec{y} + X^T X \\vec{w} = 0 \\\\\n",
    "&\\Leftrightarrow& X^T X \\vec{w} = X^T \\vec{y} \\\\\n",
    "&\\Leftrightarrow& \\vec{w} = \\left(X^T X\\right)^{-1} X^T \\vec{y}\n",
    "\\end{array}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gauss–Markov theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem\n",
    "\n",
    "In statistics, the Gauss–Markov theorem states that the ordinary least squares (OLS) estimator has the lowest sampling variance within the class of linear unbiased estimators, if the errors in the linear regression model are\n",
    "\n",
    "* uncorrelated\n",
    "* have equal variances (homoscedasticity)\n",
    "* expectation value of zero\n",
    "\n",
    "The errors do not need to be normal, nor do they need to be independent and identically distributed (only uncorrelated with mean zero and homoscedastic with finite variance). The requirement that the estimator be unbiased cannot be dropped, since biased estimators exist with lower variance. See, for example, ridge regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Maximum Likelihood Estimation\n",
    "\n",
    "https://www.kaggle.com/kashnitsky/topic-4-linear-models-part-1-ols\n",
    "\n",
    "$$\\Large \\textbf y = \\textbf X \\textbf w + \\epsilon,$$\n",
    "\n",
    "Assume that the random errors follow a centered <a href=\"https://en.wikipedia.org/wiki/Normal_distribution\">Normal distribution</a>:\n",
    "\n",
    "$$\\Large \\epsilon_i \\sim \\mathcal{N}\\left(0, \\sigma^2\\right)$$\n",
    "\n",
    "Let's rewrite the model from a new perspective:\n",
    "\n",
    "$$\\Large \\begin{array}{rcl} \n",
    "y_i &=& \\sum_{j=1}^m w_j X_{ij} + \\epsilon_i \\\\\n",
    "&\\sim& \\sum_{j=1}^m w_j X_{ij} + \\mathcal{N}\\left(0, \\sigma^2\\right) \\\\\n",
    "p\\left(y_i \\mid \\textbf X; \\textbf{w}\\right) &=& \\mathcal{N}\\left(\\sum_{j=1}^m w_j X_{ij}, \\sigma^2\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "Since the examples are taken independently (uncorrelated errors is one of the conditions of the Gauss-Markov theorem), the full likelihood of the data will look like a product of the density functions $p\\left(y_i\\right)$. Let's consider the log-likelihood, which allows us to switch products to sums:\n",
    "\n",
    "$$\\Large \\begin{array}{rcl} \n",
    "\\log p\\left(\\textbf{y}\\mid \\textbf X; \\textbf{w}\\right) &=& \\log \\prod_{i=1}^n \\mathcal{N}\\left(\\sum_{j=1}^m w_j X_{ij}, \\sigma^2\\right) \\\\\n",
    "&=& \\sum_{i=1}^n \\log \\mathcal{N}\\left(\\sum_{j=1}^m w_j X_{ij}, \\sigma^2\\right) \\\\\n",
    "&=& \\sum_{i=1}^n \\log (разобраться)\\\\\n",
    "&=& -\\frac{n}{2}\\log 2\\pi\\sigma^2 -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n \\left(y_i - \\textbf{w}^\\text{T} \\textbf{x}_i\\right)^2\n",
    "\\end{array}$$\n",
    "\n",
    "We want to find the maximum likelihood hypothesis i.e. we need to maximize the expression $p\\left(\\textbf{y} \\mid \\textbf X; \\textbf{w}\\right)$ to get $\\textbf{w}_{\\text{ML}}$, which is the same as maximizing its logarithm. Note that, while maximizing the function over some parameter, you can throw away all the members that do not depend on this parameter:\n",
    "\n",
    "$$\\Large \\begin{array}{rcl} \n",
    "\\textbf{w}_{\\text{ML}} &=& \\arg \\max_{\\textbf w} p\\left(\\textbf{y}\\mid \\textbf X; \\textbf{w}\\right) = \\arg \\max_{\\textbf w} \\log p\\left(\\textbf{y}\\mid \\textbf X; \\textbf{w}\\right)\\\\\n",
    "&=& \\arg \\max_{\\textbf w} -\\frac{n}{2}\\log 2\\pi\\sigma^2 -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n \\left(y_i - \\textbf{w}^{\\text{T}} \\textbf{x}_i\\right)^2 \\\\\n",
    "&=& \\arg \\max_{\\textbf w} -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n \\left(y_i - \\textbf{w}^{\\text{T}} \\textbf{x}_i\\right)^2 \\\\\n",
    "&=&  \\arg \\min_{\\textbf w} \\mathcal{L}\\left(\\textbf X, \\textbf{y}, \\textbf{w} \\right)\n",
    "\\end{array}$$\n",
    "\n",
    "Thus, we have seen that the maximization of the likelihood of data is the same as the minimization of the mean squared error (given the above assumptions). It turns out that such a cost function is a consequence of the fact that the errors are distributed normally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input X\n",
    "A = Matrix([[1,1],[1,2],[1,3],[1,4],[1,5],[1,6],[1,7],[1,8],[1,9],[1,10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "    X^{T}:\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1\\\\1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[1, 1, 1, 1, 1, 1, 1, 1, 1,  1],\n",
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(A.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input y\n",
    "y = Matrix([[0.59],[0.72],[0.93],[1.27],[2.58],[1.74],[2.07],[2.42],[2.73],[2.95]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "    y^{T}:\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0.59 & 0.72 & 0.93 & 1.27 & 2.58 & 1.74 & 2.07 & 2.42 & 2.73 & 2.95\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([[0.59, 0.72, 0.93, 1.27, 2.58, 1.74, 2.07, 2.42, 2.73, 2.95]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(y.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = A.transpose()\n",
    "d = (z*A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "(X^{T} \\cdot X)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}10 & 55\\\\55 & 385\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[10,  55],\n",
       "[55, 385]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = d.inv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "(X^{T} \\cdot X)^{-1}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}\\frac{7}{15} & - \\frac{1}{15}\\\\- \\frac{1}{15} & \\frac{2}{165}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[ 7/15, -1/15],\n",
       "[-1/15, 2/165]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = l*A.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "(X^{T} \\cdot X)^{-1} \\cdot X^{T}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}\\frac{2}{5} & \\frac{1}{3} & \\frac{4}{15} & \\frac{1}{5} & \\frac{2}{15} & \\frac{1}{15} & 0 & - \\frac{1}{15} & - \\frac{2}{15} & - \\frac{1}{5}\\\\- \\frac{3}{55} & - \\frac{7}{165} & - \\frac{1}{33} & - \\frac{1}{55} & - \\frac{1}{165} & \\frac{1}{165} & \\frac{1}{55} & \\frac{1}{33} & \\frac{7}{165} & \\frac{3}{55}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[  2/5,    1/3,  4/15,   1/5,   2/15,  1/15,    0, -1/15, -2/15, -1/5],\n",
       "[-3/55, -7/165, -1/33, -1/55, -1/165, 1/165, 1/55,  1/33, 7/165, 3/55]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = f*y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\vec{w} = \\left(X^T X\\right)^{-1} X^T \\vec{y}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0.322666666666667\\\\0.268606060606061\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[0.322666666666667],\n",
       "[0.268606060606061]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trend forecasting ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "y_{11 tr} = \\hat{w}_1 + \\hat{w}_2 \\cdot 11\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y11tr =  3.27733333333333\n"
     ]
    }
   ],
   "source": [
    "y11tr = u[0]+u[1]*11\n",
    "print('y11tr = ', y11tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "{\\hat{\\sigma}_{tr}}^2 = \\frac{1}{8} \\cdot \\sum^{10}_{k=1}\n",
    "({y_k - \\hat{w}_{1} - \\hat{w}_2 \\cdot k})^2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RSS(b0, b1, y):\n",
    "    RSS = 0\n",
    "    RSSi = 0\n",
    "    for i in range(1, len(y)+1):\n",
    "        RSSi = (y[i-1]-b0-b1*i)**2\n",
    "        RSS += RSSi\n",
    "    D = RSS/(len(y)-2)\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D =  0.121586212121212\n"
     ]
    }
   ],
   "source": [
    "b0 = u[0]\n",
    "b1 = u[1]\n",
    "D = RSS(b0,b1,y)\n",
    "print('D = ', D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expert forecasting ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y11 = 3.42 # expert estimation\n",
    "s11 = 0.07 # standart deviation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint forecasting ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "y_{for 11} = \\frac{\n",
    "    \\frac{y_{11}}{s_{11}^2}+\\frac{y_{11tr}}{ \\hat{\\sigma}_{tr}^2}\n",
    "    }\n",
    "    {\n",
    "    \\frac{1}{s_{11}^2}+\\frac{1}{ \\hat{\\sigma}_{tr}^2}\n",
    "    }\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint forecasting:  3.41447317889481\n"
     ]
    }
   ],
   "source": [
    "yfor11 = ((y11)/(s11**2)+(y11tr)/(D))/((1)/(s11**2)+(1)/(D))\n",
    "print(\"Joint forecasting: \", yfor11)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
